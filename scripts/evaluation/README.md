# `eval_sampling.py`

This script analyzes and visualizes the results of Active Testing or random sampling experiments. It uses the `get_metric_stats` function from utils.py to compute statistics (mean, std, quantiles, etc.) for evaluation metrics (accuracy, loss, etc.) as a function of the number of labeled samples. It also generates plots comparing the performance of sampled subsets to the full dataset.


## Arguments:

- `<metrics_dir>`:
  - Path to the directory containing metrics files generated by sample.py.
  - This directory should contain multiple metrics_*.csv files (one per experiment run) and a metrics_full.json file with metrics for the full dataset.

## What the script does:

- Loads all metrics CSV files and the full dataset metrics from the specified directory.
- Computes statistics (mean, median, std, quantiles, MSE, etc.) for each metric at each sampling step using get_metric_stats.
- Optionally generates and saves plots showing:
    - How well metrics (accuracy, loss, etc.) are estimated as more samples are labeled.
    - The difference between sampled subset metrics and full dataset metrics.
    - Quantile bands and error bars for each metric.
- Saves computed statistics and plots in the same directory.

## Output:

- Plots: `metrics_<metric>.png`, `differences_<metric>.png` (for each metric)
- Full dataset statistics: Saved as a JSON file

## Example:
```
python eval_sampling.py outputs/reddit_dataset_12/one-vs-all/football/distilbert-base-uncased/football_distilbert_one_vs_all/eval_outputs/football_one_vs_all_balanced/info_gain_sampling_outputs
```

## Notes:

- The script expects the metrics directory to be structured as output by sample.py.

- For details on the statistics and plotting, see [`get_metric_stats`](https://github.com/alan-turing-institute/ARC-TIGERS/blob/d40b20bc876e31ee58beadbef4f83b18d883366c/src/arc_tigers/eval/utils.py#L141) in `arc_tiger/eval/utils.py`.

# `sampling_error.py`

This script compares the sampling error (mean squared error, MSE) of different sampling or acquisition strategies as the number of labeled samples increases. It uses metric statistics (such as MSE) computed by get_metric_stats from arc_tigers.eval.utils and generates plots to visualize the error and improvement over random sampling.

## Arguments:

- <directory>:
  - Path to the directory containing /eval_outputs/ with experiment results.
- <save_folder>:
  - Name for the combination of experiments (e.g., 'reddit_balanced'); figures will be saved under <directory>/figures/<save_folder>/.
- --experiments:
  - List of experiment subdirectories within /eval_outputs/ (e.g., random_sampling, cross_entropy, accuracy) containing metrics CSV files.

## What the script does:

- Loads metric statistics for each experiment using get_metric_stats.
- Plots the MSE (difference to full test set) for each metric and experiment as the number of labeled samples increases.
- Plots the improvement in MSE for each experiment relative to random sampling.
- Saves all plots as PNG files in the specified figures directory.

## Output:

- MSE plots: mse_<metric>.png for each metric and experiment.
- Improvement plots: improvement_<metric>.png showing improvement over random sampling.
- All figures are saved in <directory>/figures/<save_folder>/.

## Example:

To compare distance-based sampling to random sampling, the following command can be used. Full pathnames have been removed for brevity.

```
python sampling_error.py outputs/reddit_dataset_12/one-vs-all/football/distilbert-base-uncased/football_distilbert_one_vs_all/ sport_one_vs_all_balanced --experiments <full_path_to_random_sampling_output_folder> <full_path_to_distance_based_sampling_output_folder>
```

## Notes:

- Each experiment directory should contain metrics CSV files (e.g., metrics_*.csv) as produced by sample.py.
- The script expects the first experiment in --experiments to be the baseline (e.g., random sampling).
- For details on metric computation see [`get_metric_stats`](https://github.com/alan-turing-institute/ARC-TIGERS/blob/d40b20bc876e31ee58beadbef4f83b18d883366c/src/arc_tigers/eval/utils.py#L141) in `arc_tiger/eval/utils.py`.

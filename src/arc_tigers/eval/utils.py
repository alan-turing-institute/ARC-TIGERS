import json
import logging
from glob import glob
from typing import Any

import matplotlib.pyplot as plt
import numpy as np
import numpy.typing as npt
import pandas as pd
import torch
from sklearn.metrics import (
    accuracy_score,
    average_precision_score,
    classification_report,
    precision_recall_fscore_support,
)
from torch.nn.functional import cross_entropy

logger = logging.getLogger(__name__)


def plot_difference(
    save_dir: str,
    metric_stats: dict[
        str, npt.NDArray[np.integer] | dict[str, npt.NDArray[np.integer | np.floating]]
    ],
    full_metrics: dict[str, int | float],
):
    for metric in metric_stats:
        if metric == "n_labels":
            continue

        if "n_class" in metric:
            continue

        plt.axhline(y=0, color="k", linestyle="--", alpha=0.2)
        diff = metric_stats[metric]["mean"] - full_metrics[metric]
        diff_min = (
            metric_stats[metric]["mean"] - metric_stats[metric]["std"]
        ) - full_metrics[metric]
        diff_max = (
            metric_stats[metric]["mean"] + metric_stats[metric]["std"]
        ) - full_metrics[metric]

        plt.plot(
            metric_stats["n_labels"],
            diff,
            label="Mean (labelled subset)",
            linestyle="-",
            linewidth=2,
        )
        plt.fill_between(
            metric_stats["n_labels"],
            diff_min,
            diff_max,
            linestyle="-",
            alpha=0.3,
            linewidth=2,
        )

        plt.ylabel(f"Difference to full test {metric}")
        plt.xlabel("Number of labelled samples")
        plt.legend()
        plt.tight_layout()
        plt.savefig(f"{save_dir}/differences_{metric}.png", dpi=300)
        plt.close()


def plot_metrics(
    save_dir: str,
    metric_stats: dict[
        str, npt.NDArray[np.integer] | dict[str, npt.NDArray[np.integer | np.floating]]
    ],
    full_metrics: dict[str, int | float],
    quantiles: list[float] | None = None,
    quantile_colors: list[str] | None = None,
):
    """Plot how metric stats vary with the number of labelled samples.

    Args:
        save_dir: Directory to save the plots.
        metric_stats: Dictionary containing the metric stats.
        full_metrics: Dictionary containing the metrics of the full dataset.
        quantiles: List of quantiles to plot. If None, defaults to [0.025, 0.25].
        quantile_colors: List of colors for the quantiles. If None, defaults to ["r",
            "orange"].
    """
    if quantile_colors is None:
        quantile_colors = ["r", "orange"]
    if quantiles is None:
        quantiles = [0.025, 0.25]

    limits_dict = {
        "accuracy": (0.5, 1.0),
        "loss": (0.0, 2.0),
        "n_class_0": (0, np.max(metric_stats["n_labels"])),
        "n_class_1": (0, np.max(metric_stats["n_labels"])),
    }

    for metric in metric_stats:
        if metric == "n_labels":
            continue
        if "n_class" not in metric:
            # n_class on full dataset is just the full dataset size, which isn't helpful
            # to plot
            plt.axhline(
                y=full_metrics[metric],
                color="k",
                linestyle="--",
                label="Ground Truth",
            )

        plt.plot(
            metric_stats["n_labels"],
            metric_stats[metric]["mean"],
            label="Mean (labelled subset)",
            color="blue",
            linestyle="-",
            linewidth=2,
        )
        for quantile, color in zip(quantiles, quantile_colors, strict=True):
            plt.fill_between(
                metric_stats["n_labels"],
                metric_stats[metric][f"quantile_{quantile * 100}"],
                metric_stats[metric][f"quantile_{(1 - quantile) * 100}"],
                color=color,
                alpha=0.2,
                label=f"Quantiles ({quantile * 100} - {(1 - quantile) * 100})",
            )
        y_min = limits_dict.get(metric, (-0.1, 1.1))[0]
        y_max = limits_dict.get(metric, (-0.1, 1.1))[1]
        plt.ylim(y_min, y_max)
        plt.ylabel(metric)
        plt.xlabel("Number of labelled samples")
        plt.legend()
        plt.tight_layout()
        plt.savefig(f"{save_dir}/metrics_{metric}.png", dpi=300)
        plt.close()


def get_metric_stats(
    data_dir: str, plot: bool = True
) -> tuple[dict[str, Any | dict[str, Any]], Any]:
    """
    Compute and plot stats of metric values after a varying number of labelled samples.

    Args:
        data_dir: Directory containing the metrics files generated by
            random_sampling.py. Files should be named `metrics_*.csv` with each file
            containing metric values for a single loop of sampling. It should also
            contain a `metrics_full.json` file with the metrics calculated for the whole
            dataset. This is used to compare the metrics on the labelled subset with the
            metrics on the whole dataset.

    Returns:
        A dictionary containing the stats of the metrics. The keys are the metric names
        and the values are dictionaries containing the mean, median, std, and quantiles
        of the metric values for each number of labelled samples. There is also a
        parent `n_labels` key containing the number of labelled samples after each
        metric calculation.
    """
    result_files = glob(f"{data_dir}/metrics_*.csv")
    if len(result_files) == 0:
        msg = "No metrics files found in the directory."
        raise ValueError(msg)
    repeats = [pd.read_csv(f, index_col="n") for f in result_files]
    if any(len(r) != len(repeats[0]) for r in repeats):
        msg = "Not all files have the same number of rows."
        raise ValueError(msg)
    if any(list(r.columns) != list(repeats[0].columns) for r in repeats):
        msg = "Not all files have the same columns."
        raise ValueError(msg)

    with open(f"{data_dir}/metrics_full.json") as f:
        full_metrics = json.load(f)
    if set(repeats[0].columns) != set(full_metrics.keys()):
        msg = "Metrics in full metrics file do not match those in the CSV files."
        raise ValueError(msg)

    metrics = repeats[0].columns
    metric_stats: dict[
        str, npt.NDArray[np.integer] | dict[str, npt.NDArray[np.integer | np.floating]]
    ] = {}
    metric_stats["n_labels"] = np.array(repeats[0].index)
    n_repeats = len(repeats)
    n_samples = len(repeats[0])
    quantiles = [0.025, 0.25]  # will also compute 1 - quantiles
    quantile_colors = ["r", "orange"]
    for metric in metrics:
        metric_repeats = np.full((n_repeats, n_samples), np.nan)
        for idx, r in enumerate(repeats):
            metric_repeats[idx, :] = r[metric]

        metric_stats[metric] = {}
        metric_stats[metric]["mean"] = np.mean(metric_repeats, axis=0)
        metric_stats[metric]["median"] = np.median(metric_repeats, axis=0)
        metric_stats[metric]["std"] = np.std(metric_repeats, axis=0)
        metric_stats[metric]["mse"] = np.mean(
            (metric_repeats - full_metrics[metric]) ** 2, axis=0
        )
        metric_stats[metric]["mse_std"] = np.std(
            (metric_repeats - full_metrics[metric]) ** 2, axis=0
        )

        for quantile in quantiles:
            metric_stats[metric][f"quantile_{quantile * 100}"] = np.quantile(
                metric_repeats, quantile, axis=0
            )
            metric_stats[metric][f"quantile_{(1 - quantile) * 100}"] = np.quantile(
                metric_repeats, 1 - quantile, axis=0
            )
        metric_stats[metric]["IQR"] = (
            metric_stats[metric]["quantile_75.0"]
            - metric_stats[metric]["quantile_25.0"]
        )

    if plot:
        plot_metrics(
            data_dir,
            metric_stats,
            full_metrics,
            quantiles,
            quantile_colors,
        )
        plot_difference(
            data_dir,
            metric_stats,
            full_metrics,
        )

    return metric_stats, full_metrics


class BiasCorrector:
    """
    Implements a bias correction weighting factor for active learning sampling.

    The BiasCorrector computes a weighting factor (v_m) for a selected sample
    during active learning, as described in the formula:

        v_m = 1 + (N - M) / (N - m) * (1 / ((N - m + 1) * q_im) - 1)

    where:
        N    : Total number of samples in the dataset.
        M    : Total number of samples to be labelled (target number).
        m    : Number of samples labelled so far.
        q_im : The probability (from the acquisition function's pmf) of selecting the
        sample at step m.

    This correction is used to adjust for the sampling bias introduced by non-uniform
    acquisition functions, ensuring unbiased estimation of metrics or statistics
    over the dataset.

    Args:
        N (int): Total number of samples in the dataset.
        M (int): Total number of samples to be labelled.

    Methods:
        call(q_im, m): Computes the weighting factor for the selected sample.
    """

    def __init__(self, N, M):
        self.N = N
        self.M = M
        self.v_values = []

    def compute_weighting_factor(self, q_im, m):
        """
        Compute the weighting factor v_m for the selected sample.

        Args:
            q_im: The pmf value for the selected sample (float)
            N: Total number of samples (int)
            M: Total number of samples to be labelled (int)
            m: Number of samples labelled so far (int)

        Returns:
            v_m: Weighting factor (float)
        """
        inner = (1 / ((self.N - m + 1) * q_im)) - 1
        v_m = 1 + ((self.N - self.M) / (self.N - m)) * inner
        self.v_values.append(v_m)
        return v_m

    def apply_weighting_to_dict(
        self, q: float, m: int, metrics: dict[str, float]
    ) -> dict[str, float]:
        """
        Apply the weighting factor to all values in the metrics dictionary.

        Args:
            q: The pmf value for the selected sample (float)
            m: Number of samples labelled so far (int)
            metrics: Dictionary of metric values to be weighted {metric_name: value}

        Returns:
            dict: Dictionary with weighted metric values.
        """

        weighting = self.compute_weighting_factor(q, m)
        return {k: v * weighting for k, v in metrics.items()}


def compute_loss(
    logits: np.ndarray,
    labels: np.ndarray,
    sample_weight: np.ndarray | None = None,
) -> float:
    """
    Compute the cross-entropy loss between logits and labels.

    Args:
        logits: logits from the model, shape (n_samples, n_classes)
        labels: labels for the dataset, shape (n_samples,)

    Returns:
        loss: The computed cross-entropy loss.
    """
    # compute cross-entropy loss
    if sample_weight:
        # apply bias correction to logits
        loss = cross_entropy(
            torch.tensor(logits, dtype=torch.float32),
            torch.tensor(labels),
            reduction="none",
        )
        loss = (loss * torch.tensor(sample_weight, dtype=loss.dtype)).mean()
    else:
        loss = cross_entropy(
            torch.tensor(logits, dtype=torch.float32),
            torch.tensor(labels),
            reduction="mean",
        )
    return loss.item()


def evaluate(
    dataset,
    preds,
    bias_corrector: BiasCorrector | None = None,
) -> dict[str, Any]:
    """
    Compute metrics for a given dataset with preds.

    Args:
        dataset: The dataset to compute metrics for
        preds: The predictions for the dataset.
        model: The model to compute metrics with.

    Returns:
        A dictionary containing the computed metrics.
    """
    # Placeholder for actual metric computation
    eval_pred = (preds, dataset["label"])
    metrics = compute_metrics(eval_pred, bias_corrector=bias_corrector)
    metric_names = list(metrics.keys())
    for key in metric_names:
        if isinstance(metrics[key], list):
            # unpack multi-valued metrics into separate values for each class
            multi_metric = metrics.pop(key)
            if len(multi_metric) == 1:
                msg = "If metric value is a list, should have more than 1 value"
                raise ValueError(msg)
            for i, m in enumerate(multi_metric):
                metrics[f"{key}_{i}"] = m

    return metrics


# Define metrics
def compute_metrics(
    eval_pred: tuple[np.ndarray, np.ndarray],
    bias_corrector: BiasCorrector | None = None,
) -> dict[str, Any]:
    logits, labels = eval_pred
    if logits.ndim == 1:
        # Allow logits to be passed as the predictions
        if np.issubdtype(logits.dtype, np.integer):
            predictions = logits
            probs = logits  # fallback, not ideal
        else:
            predictions = (logits > 0.5).astype(int)
            probs = logits
    else:
        probs = softmax(logits)[:, 1] if logits.shape[1] > 1 else softmax(logits)[:, 0]
        predictions = np.argmax(logits, axis=-1)

    sample_weight = bias_corrector.v_values if bias_corrector else None
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels,
        predictions,
        labels=[0, 1],  # assumed binary labels (0 and 1) here
        sample_weight=sample_weight,
    )
    acc = accuracy_score(
        labels,
        predictions,
        sample_weight=sample_weight,
    )

    loss = compute_loss(logits=logits, labels=labels, sample_weight=sample_weight)

    # Improved Average Precision (AP) handling
    unique_labels = np.unique(labels)
    if len(unique_labels) < 2:
        avg_precision = float("nan")
        warning = (
            "average_precision_score is undefined: only one class"
            f" ({unique_labels[0]}) present in labels."
        )
        logger.warning(warning)
    else:
        avg_precision = average_precision_score(
            labels, probs, sample_weight=sample_weight
        )

    # no. of samples per class in the eval data (also assumed binary labels here,
    # minlength should be set to the number of classes to ensure
    # len(samples_per_class) == n_classes)
    if isinstance(labels, torch.Tensor):
        n_class = torch.bincount(labels, minlength=2)
    else:
        n_class = np.bincount(labels, minlength=2)

    eval_scores = {
        "accuracy": acc,
        "f1": f1.tolist(),
        "precision": precision.tolist(),
        "recall": recall.tolist(),
        "loss": loss,
        "average_precision": avg_precision,
        "n_class": n_class.tolist(),
    }
    logger.info("Eval metrics: %s", eval_scores)
    return eval_scores


def softmax(logits: np.ndarray) -> np.ndarray:
    """
    Compute the softmax of the logits.

    Args:
        logits: The logits to compute the softmax for.

    Returns:
        The softmax of the logits.
    """
    return np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)


def entropy(logits: np.ndarray) -> np.ndarray:
    """
    Compute the entropy of the logits.

    Args:
        logits: The logits to compute the entropy for.

    Returns:
        The entropy of the logits.
    """
    softmax_probs = softmax(logits)
    return -np.sum(softmax_probs * np.log(softmax_probs + 1e-10), axis=-1)


def get_stats(preds, labels):
    """
    Compute statistics for a given dataset with preds.

    Args:
        preds: The labels for the dataset.
        labels: The dataset to compute metrics for

    Returns:
        A dictionary containing the computed metrics.
    """
    # Placeholder for actual metric computation
    logits = np.array(preds)
    class_preds = np.argmax(logits, axis=-1)
    label_vector = np.array(labels)

    accuracy_vector = np.array(class_preds == label_vector, dtype=int)
    entropy_values = entropy(logits)

    return {
        "accuracy": accuracy_vector.tolist(),
        "softmax": softmax(logits).tolist(),
        "entropy": entropy_values.tolist(),
    }


def tfidf_evaluation(eval_labels, eval_predictions):
    eval_results = classification_report(
        eval_labels, eval_predictions, output_dict=True
    )
    output_dict = {}
    output_dict["eval_accuracy"] = eval_results["accuracy"]
    n_classes = len(eval_results) - 3
    output_dict["eval_f1"] = [
        eval_results[str(i)]["f1-score"] for i in range(n_classes)
    ]
    output_dict["eval_precision"] = [
        eval_results[str(i)]["precision"] for i in range(n_classes)
    ]
    output_dict["eval_recall"] = [
        eval_results[str(i)]["recall"] for i in range(n_classes)
    ]
    return output_dict
